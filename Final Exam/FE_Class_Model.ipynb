{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport numpy.random as nr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn import preprocessing\nimport sklearn.model_selection as ms\nfrom sklearn import feature_selection as fs\nfrom sklearn import linear_model\nimport sklearn.metrics as sklm\nimport scipy.stats as ss\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Bike_Buyer_Preped.csv')\ndf.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(df['BikeBuyer'].head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inv_BB_mapping = {'yes' : 1, 'no' : 0}\ndf['BikeBuyer']=df['BikeBuyer'].map(inv_BB_mapping)\nprint(df['BikeBuyer'].head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "labels = np.array(df['BikeBuyer'])            # encode label\nprint(labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def encode_string(cat_features):\n    ## First encode the strings to numeric categories\n    enc = preprocessing.LabelEncoder()\n    enc.fit(cat_features)\n    enc_cat_features = enc.transform(cat_features)\n    ## Now, apply one hot encoding\n    ohe = preprocessing.OneHotEncoder()\n    encoded = ohe.fit(enc_cat_features.reshape(-1,1))\n    return encoded.transform(enc_cat_features.reshape(-1,1)).toarray()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "categorical_columns = ['Education', 'Occupation', 'Gender', 'MaritalStatus']\nFeatures = encode_string(df['CountryRegionName'])\nfor col in categorical_columns:\n    temp = encode_string(df[col])\n    Features = np.concatenate([Features, temp], axis = 1)\n\nprint(Features.shape)\nprint(Features[:2, :])    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Features = np.concatenate([Features, np.array(df[['NumberCarsOwned', 'NumberChildrenAtHome', \n                            'TotalChildren', 'log_Income','log_Age','log_AveMonthSpend']])], axis = 1)\nprint(Features.shape)\nprint(Features[:2, :])  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(9988)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = int(0.3*df.shape[0]))\nprint(int(0.3*df.shape[0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train = Features[indx[0],:]\ny_train = np.ravel(labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(labels[indx[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler = preprocessing.StandardScaler().fit(X_train[:,25:])\nX_train[:,25:] = scaler.transform(X_train[:,25:])\nX_test[:,25:] = scaler.transform(X_test[:,25:])\nX_train[:2,]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "logistic_mod = linear_model.LogisticRegression() \nlogistic_mod.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(logistic_mod.intercept_)\nprint(logistic_mod.coef_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "probabilities = logistic_mod.predict_proba(X_test)\nprint(probabilities[:15,:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\nscores = score_model(probabilities, 0.5)\nprint(np.array(scores[:15]))\nprint(y_test[:15])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_metrics(labels, scores):\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d'   % metrics[3][0] + '        %6d'   % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\nprint_metrics(y_test, scores)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1]) # requires the probs evaluated above\n    auc = sklm.auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \nplot_auc(y_test, probabilities)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "logistic_mod = linear_model.LogisticRegression(class_weight = {0:0.45, 1:0.55}) \nlogistic_mod.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "probabilities = logistic_mod.predict_proba(X_test)\nprint(probabilities[:15,:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scores = score_model(probabilities, 0.5)\nprint_metrics(y_test, scores)  \nplot_auc(y_test, probabilities)  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def test_threshold(probs, labels, threshold):\n    scores = score_model(probs, threshold)\n    print('')\n    print('For threshold = ' + str(threshold))\n    print_metrics(labels, scores)\n\nthresholds = [0.45, 0.40, 0.35, 0.3, 0.25]\nfor t in thresholds:\n    test_threshold(probabilities, y_test, t)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\n# perform feature selection\n#\nprint(Features.shape)\nprint(labels.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Define the variance threshold and fit the threshold to the feature array. \nsel = fs.VarianceThreshold(threshold=(.8 * (1 - .8)))\nFeatures_reduced = sel.fit_transform(Features)\n\n## Print the support and shape for the transformed features\nprint(sel.get_support())\nprint(Features_reduced.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Reshape the Label array\nLabels = labels.reshape(labels.shape[0],)\n\n## Set folds for nested cross validation\nnr.seed(988)\nfeature_folds = ms.KFold(n_splits=10, shuffle = True)\n\n## Define the model\nlogistic_mod = linear_model.LogisticRegression(C = 10, class_weight = {0:0.45, 1:0.55}) \n\n## Perform feature selection by CV with high variance features only\nnr.seed(6677)\nselector = fs.RFECV(estimator = logistic_mod, cv = feature_folds,\n                      scoring = 'roc_auc')\nselector = selector.fit(Features_reduced, labels)\nselector.support_ ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "selector.ranking_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Features_reduced = selector.transform(Features_reduced)\nFeatures_reduced.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.title('Mean AUC by number of features')\nplt.ylabel('AUC')\nplt.xlabel('Number of features')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(123)\ninside = ms.KFold(n_splits=10, shuffle = True)\nnr.seed(321)\noutside = ms.KFold(n_splits=10, shuffle = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(3456)\n## Define the dictionary for the grid search and the model object to search on\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000]}\n## Define the logistic regression model\nlogistic_mod = linear_model.LogisticRegression(class_weight = {0:0.45, 1:0.55}) \n\n## Perform the grid search over the parameters\nclf = ms.GridSearchCV(estimator = logistic_mod, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'roc_auc',\n                      return_train_score = True)\n\n## Fit the cross validated grid search over the data \nclf.fit(Features_reduced, Labels)\n\n## And print the best parameter value\nclf.best_estimator_.C",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_cv(clf, params_grid, param = 'C'):\n    params = [x for x in params_grid[param]]\n  \n    keys = list(clf.cv_results_.keys())              \n    grid = np.array([clf.cv_results_[key] for key in keys[6:16]])\n    means = np.mean(grid, axis = 0)\n    stds = np.std(grid, axis = 0)\n    print('Performance metrics by parameter')\n    print('Parameter   Mean performance   STD performance')\n    for x,y,z in zip(params, means, stds):\n        print('%8.2f        %6.5f            %6.5f' % (x,y,z))\n    \n    params = [math.log10(x) for x in params]\n    \n    plt.scatter(params * grid.shape[0], grid.flatten())\n    p = plt.scatter(params, means, color = 'red', marker = '+', s = 300)\n    plt.plot(params, np.transpose(grid))\n    plt.title('Performance metric vs. log parameter value\\n from cross validation')\n    plt.xlabel('Log hyperparameter value')\n    plt.ylabel('Performance metric')\n    \nplot_cv(clf, param_grid)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(498)\ncv_estimate = ms.cross_val_score(clf, Features, Labels, \n                                 cv = outside) # Use the outside folds\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Test the model\n## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features_reduced.shape[0])\nindx = ms.train_test_split(indx, test_size = 4200)\nX_train = Features_reduced[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features_reduced[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n\n## Define and fit the logistic regression model\nlogistic_mod = linear_model.LogisticRegression(C = 1, class_weight = {0:0.35, 1:0.65}) \nlogistic_mod.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\nprobabilities = logistic_mod.predict_proba(X_test)\nprint(probabilities[:15,:])\nscores = score_model(probabilities, 0.3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d'   % metrics[3][0] + '        %6d'   % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n\ndef plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n    auc = sklm.auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()    \n        \nprint_metrics(y_test, probabilities, 0.3)    \nplot_auc(y_test, probabilities)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}